{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francesco-source/DeepLearning/blob/main/image_inpainting/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image inpainting problem on CIFAR10\n",
        "\n",
        "In this notebook I try to implement different nets for solving the same problem.\n",
        "\n",
        "This was done to fine-tune the networks for optimal performance and to learn the majority of the theoretical concepts covered in the course."
      ],
      "metadata": {
        "id": "FC0L-9_sBuZz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmG-CJXKa0YJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models, metrics\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras import callbacks\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(\"image range is {}, {}\".format(np.min(x_test,axis=(0,1,2,3)),np.max(x_test,axis=(0,1,2,3))))\n",
        "x_train = (x_train/255.).astype(np.float32)\n",
        "x_test = (x_test/255.).astype(np.float32)\n",
        "print(\"new image range is {}, {}\".format(np.min(x_test,axis=(0,1,2,3)),np.max(x_test,axis=(0,1,2,3))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm9wzzKNbJtF",
        "outputId": "b74caf1b-47f7-4488-abe8-5fc2cc38ad28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "image range is 0, 255\n",
            "new image range is 0.0, 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask(X,coords):\n",
        "  x0,y0,x1,y1 = coords\n",
        "  X[:,x0:x1,y0:y1] = 0\n",
        "  return X\n",
        "\n",
        "\n",
        "masked_x_train = mask(np.copy(x_train),(2,16,30,30))\n",
        "masked_x_test = mask(np.copy(x_test),(2,16,30,30))\n"
      ],
      "metadata": {
        "id": "vlRo8Ij7bKjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder for image unpainting"
      ],
      "metadata": {
        "id": "kqGphH35bUHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This neural network is a convolutional autoencoder model. The goal of the convolutional autoencoder is to compress an input image into a low-dimensional representation (often called a \"latent code\") and then reconstruct the input image from the latent code.\n",
        " \n",
        " Before the autoencoder we have 2 indipendent convolution blocks in order to learn different features from the input image.\n",
        " There is no dense layer as a bottleneck beacuse the dataset is small compared to the size of the net. \n",
        "\n",
        "The convolutional autoencoder is composed of two main parts:\n",
        "1. The encoder consists of a series of convolutional layers that compress the input image into a low-dimensional representation.\n",
        "2. The decoder, on the other hand, uses a series of deconvolution (or upsampling) layers to reconstruct the input image from the low-dimensional representation.\n",
        "\n",
        "The neural network shown here begins with a first convolutional layer that uses a 5x5 convolution and a stride of 1. This layer is followed by a set of four convolutional blocks, each consisting of three convolutional layers, a LeakyReLU activation function with an alpha of 0.2, and a batch normalization layer. The LeakyReLu is used in order to prevent the vanishing relu problem. \n",
        "These convolutional blocks are designed to extract the features of the input image in order to compress them into a low-dimensional representation.\n",
        "\n",
        "Subsequently, the neural network uses three deconvolution (or upsampling) blocks, each consisting of a deconvolution layer, a LeakyReLU activation function, a batch normalization layer, and a concatenation with the corresponding output of the encoder. The last layer of the decoder is a 3x3 convolution with a sigmoid activation function, which produces the reconstructed image.\n",
        "\n",
        "In summary, this neural network is designed to compress an input image into a low-dimensional representation and then reconstruct the input image from the latent code. The network uses convolutional and deconvolution blocks, LeakyReLU activation functions, and batch normalization to extract the features of the input image and reconstruct the output image."
      ],
      "metadata": {
        "id": "0VjAC_Blbhxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def ConvBlock(x, filters, kernel_size, strides=1, activation='relu', padding='same'):\n",
        "    x = Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def UpConvBlock(x, skip_connection, filters, kernel_size, strides=1, activation='relu', padding='same'):\n",
        "    x = Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Concatenate()([x, skip_connection])\n",
        "    return x\n",
        "\n",
        "def initial_differentiation(x, filters, kernel_size, strides=1, activation='relu', padding='same'):\n",
        "\n",
        "    conv_a_res = layers.Conv2D(filters=32, kernel_size=1, activation='relu', padding='same')(x)  #32, 32, 32\n",
        "    conv_a = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(conv_a_res)  #32, 32, 32\n",
        "    conv_a = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(conv_a)  #32, 32, 32\n",
        "    conv_a = layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(conv_a)  #32, 32, 32\n",
        "    add_res_a = layers.add([conv_a_res, conv_a]) #32, 32, 64\n",
        "    conv_a = layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(add_res_a) #16, 16, 32\n",
        "    norm_a = layers.BatchNormalization()(conv_a)\n",
        "\n",
        "    conv_b_res = layers.Conv2D(filters=32, kernel_size=1, activation='relu', padding='same')(x)  #32, 32, 32\n",
        "    conv_b = layers.Conv2D(filters=32, kernel_size=5, activation='relu', padding='same')(conv_b_res)  #32, 32, 32\n",
        "    conv_b = layers.Conv2D(filters=32, kernel_size=5, activation='relu', padding='same')(conv_b)  #32, 32, 32\n",
        "    conv_b = layers.Conv2D(filters=32, kernel_size=5, activation='relu', padding='same')(conv_b)  #32, 32, 32\n",
        "    add_res_b = layers.add([conv_b_res, conv_b]) #32, 32, 64\n",
        "    conv_b = layers.Conv2D(filters=64, kernel_size=5, activation='relu', padding='same')(add_res_b) #32, 32, 32\n",
        "    norm_b = layers.BatchNormalization()(conv_b)\n",
        "\n",
        "    # Concatenate the output of the two branches\n",
        "    conc_x = layers.concatenate([norm_a, norm_b]) #32, 32, 64\n",
        "    conv_x = layers.Conv2D(filters=64, kernel_size=3, padding='same')(conc_x) #32, 32, 64\n",
        "    leaky_activation = layers.LeakyReLU()(conv_x)\n",
        "\n",
        "    return leaky_activation\n",
        "\n",
        "def autoencoder(input_shape_x):\n",
        "    # Encoder\n",
        "    inputs_x = Input(shape=input_shape_x)\n",
        "    x1 = initial_differentiation(inputs_x,64,5,strides = 1, activation = \"relu\",padding = \"same\")\n",
        "    #x1 = ConvBlock(inputs_x, 64, 5, strides=1, activation='relu', padding='same')\n",
        "    x2 = ConvBlock(x1, 128, 4, strides=2, activation='relu', padding='same')\n",
        "    x3 = ConvBlock(x2, 256, 4, strides=2, activation='relu', padding='same')\n",
        "    x4 = ConvBlock(x3, 512, 4, strides=2, activation='relu', padding='same')\n",
        "\n",
        "    # Decoder\n",
        "    x = UpConvBlock(x4, x3, 256, 4, strides=2, activation='relu', padding='same')\n",
        "    x = UpConvBlock(x, x2, 128, 4, strides=2, activation='relu', padding='same')\n",
        "    x = UpConvBlock(x, x1, 64, 4, strides=2, activation='relu', padding='same')\n",
        "    x = Conv2D(3, 3, strides=1, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=inputs_x, outputs=x)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "574e6viWbi1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = autoencoder((32,32,3))\n",
        "\n",
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=76)"
      ],
      "metadata": {
        "id": "fpmkWVUZmKvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PJ-znwn8pXVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_training = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"auto\", patience=5, restore_best_weights=True)\n",
        "reduce_learning_rate = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=6, min_lr=0.000001)"
      ],
      "metadata": {
        "id": "bVN2ny8RpZxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics='accuracy')"
      ],
      "metadata": {
        "id": "YBxO35w6plSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=masked_x_train, y=x_train, validation_split=0.1, batch_size=32, epochs=100, verbose=1, shuffle=True, callbacks=[stop_training, reduce_learning_rate])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-hujoqfpypK",
        "outputId": "4bba083a-9c21-49b6-bfec-a009558f9d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1407/1407 [==============================] - 80s 50ms/step - loss: 0.0109 - accuracy: 0.7517 - val_loss: 0.0096 - val_accuracy: 0.7732 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "1407/1407 [==============================] - 70s 49ms/step - loss: 0.0096 - accuracy: 0.7908 - val_loss: 0.0105 - val_accuracy: 0.7375 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "1407/1407 [==============================] - 70s 50ms/step - loss: 0.0092 - accuracy: 0.8012 - val_loss: 0.0116 - val_accuracy: 0.7919 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0090 - accuracy: 0.8073 - val_loss: 0.0087 - val_accuracy: 0.8047 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0087 - accuracy: 0.8145 - val_loss: 0.0087 - val_accuracy: 0.8213 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "1407/1407 [==============================] - 70s 50ms/step - loss: 0.0085 - accuracy: 0.8190 - val_loss: 0.0083 - val_accuracy: 0.8342 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "1407/1407 [==============================] - 70s 50ms/step - loss: 0.0083 - accuracy: 0.8223 - val_loss: 0.0083 - val_accuracy: 0.8093 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0082 - accuracy: 0.8274 - val_loss: 0.0084 - val_accuracy: 0.8419 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "1407/1407 [==============================] - 72s 51ms/step - loss: 0.0080 - accuracy: 0.8295 - val_loss: 0.0082 - val_accuracy: 0.8264 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "1407/1407 [==============================] - 70s 50ms/step - loss: 0.0079 - accuracy: 0.8335 - val_loss: 0.0081 - val_accuracy: 0.8261 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0077 - accuracy: 0.8352 - val_loss: 0.0084 - val_accuracy: 0.8410 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0076 - accuracy: 0.8355 - val_loss: 0.0082 - val_accuracy: 0.8385 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0074 - accuracy: 0.8375 - val_loss: 0.0082 - val_accuracy: 0.8367 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0072 - accuracy: 0.8370 - val_loss: 0.0082 - val_accuracy: 0.8336 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "1407/1407 [==============================] - 69s 49ms/step - loss: 0.0070 - accuracy: 0.8391 - val_loss: 0.0084 - val_accuracy: 0.8350 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f150c3fd060>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x=masked_x_test, verbose=0)"
      ],
      "metadata": {
        "id": "5Fwi7Vvqq53q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(masked_x_test[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PpaeqLrYrDva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(predictions[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CRTDUTbu1kq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_test[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DGAUSFgh1n-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "# calculate the MSE between the predictions and ground truth\n",
        "mse = K.mean(K.square(x_test - predictions))\n",
        "# evaluate the model\n",
        "print('Mean Squared Error:', K.eval(mse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7z9ATux1sbO",
        "outputId": "77a364d8-ca08-4a44-f6e6-67bcade5556e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.008250367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse = np.mean(np.square(predictions - x_test),axis=1)\n",
        "print(mse.shape)\n",
        "print(\"The mse on the test set is : \",np.mean(mse), \" +/- \",np.std(mse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKFxdPR01vbS",
        "outputId": "c4a1fc0a-4b77-446c-9520-3571dd21df1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 32, 3)\n",
            "The mse on the test set is :  0.008207888  +/-  0.015632764\n"
          ]
        }
      ]
    }
  ]
}