{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francesco-source/DeepLearning/blob/main/mnistDense.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8wSwJnsjx5P"
      },
      "source": [
        "# Mnist classification with NNs\n",
        "A first example of a simple Neural Network, applied to a well known dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wMZ2Ge6jw1E"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import utils\n",
        "import numpy as np"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDiNppLVkvqd"
      },
      "source": [
        "Let us load the mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL8GyC0Nk14o"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijhDuLwwlQrI",
        "outputId": "187a22e4-37c3-45c7-ed07-fd725a966ce8"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(\"pixel range is [{},{}]\".format(np.min(x_train),np.max(x_train)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "pixel range is [0,255]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D01L64YcnWO5"
      },
      "source": [
        "We normalize the input in the range [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8uA2Kp7mG9s"
      },
      "source": [
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = np.reshape(x_train,(60000,28*28))\n",
        "x_test = np.reshape(x_test,(10000,28*28))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the network will be a proability distribution over the different categories. Similarly, we generate a ground truth distribution, and the training objective will consist in minimizing their distance (categorical crossentropy). The ground truth distribution is the so called \"categorical\" distribution: if x has label l, the corresponding categorical distribution has probaility 1 for the category l, and 0 for all the others."
      ],
      "metadata": {
        "id": "Yjrzmhgh3TZv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhzWUm0UnODb",
        "outputId": "868bffbc-88ae-4995-d2a8-b60519dd8a69"
      },
      "source": [
        "#we use the categoricl rapresentation of the class\n",
        "print(y_train[0])\n",
        "y_train_cat = utils.to_categorical(y_train)\n",
        "print(y_train_cat[0])\n",
        "y_test_cat = utils.to_categorical(y_test)\n",
        "# This is what we expect that the network will produce. "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfeZF3bUrFZf"
      },
      "source": [
        "Our first Netwok just implements logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBJtMj2pqJiR"
      },
      "source": [
        "xin = Input(shape=(784)) # First layer, only one layer with this shape.\n",
        "# 10 is the number of outputs of the dense layer, activation is the softmax(generalization of the logistic function).\n",
        "res = Dense(10,activation='softmax') (xin) # We apply the dense layer to the input layer. We create the layer.\n",
        "mynet = Model(inputs=xin,outputs=res) #inputs = could be a list of inputs and also outputs can be a list"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXD5JT2ZrTJc",
        "outputId": "5c0b1d8c-5b6d-44e2-9d9d-8c94ef004074"
      },
      "source": [
        "mynet.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                7850      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,850\n",
            "Trainable params: 7,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The None means that the beach dimension is unspecified. The network is able to work with any betch dimension. We have a dense network so we know that the parameters are all the possible connections. 784*10 + 1*10 biases links. "
      ],
      "metadata": {
        "id": "XIt23n2Zm9yu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcjcOz8yrk5X"
      },
      "source": [
        "Now we need to compile the network.\n",
        "In order to do it, we need to pass two mandatory arguments:\n",
        "\n",
        "\n",
        "*   the **optimizer**, in charge of governing the details of the backpropagation algorithm\n",
        "*   the **loss function**\n",
        "\n",
        "Several predefined optimizers exist, and you should just choose your favourite one. A common choice is Adam, implementing an adaptive lerning rate, with momentum\n",
        "\n",
        "Optionally, we can specify additional metrics, mostly meant for monitoring the training process.\n",
        "\n",
        "Th optimizer is usually a variant of SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XK20mAFrrkQ"
      },
      "source": [
        "mynet.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E58bT-Imvsw2"
      },
      "source": [
        "Finally, we fit the model over the trianing set. \n",
        "\n",
        "Fitting, just requires two arguments: training data e ground truth, that is x and y. Additionally we can specify epochs, batch_size, and many additional arguments.\n",
        "\n",
        "In particular, passing validation data allow the training procedure to measure loss and metrics on the validation set at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2woDXbbr6ak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e38a933-9c23-4f22-f9b2-0a121f4606cb"
      },
      "source": [
        "# batch size, amount of data that we want to process in parallel. Largest batch size possible is usually a good idea. \n",
        "# Here we are using test set as validation. Is cheading but only for teaching purpouses we do so. \n",
        "mynet.fit(x_train,y_train_cat, shuffle=True, epochs=10, batch_size=32,validation_data=(x_test,y_test_cat))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4701 - accuracy: 0.8771 - val_loss: 0.3096 - val_accuracy: 0.9150\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3040 - accuracy: 0.9154 - val_loss: 0.2823 - val_accuracy: 0.9205\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2831 - accuracy: 0.9213 - val_loss: 0.2737 - val_accuracy: 0.9254\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2727 - accuracy: 0.9237 - val_loss: 0.2686 - val_accuracy: 0.9260\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2664 - accuracy: 0.9261 - val_loss: 0.2690 - val_accuracy: 0.9247\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2616 - accuracy: 0.9278 - val_loss: 0.2644 - val_accuracy: 0.9274\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2584 - accuracy: 0.9284 - val_loss: 0.2660 - val_accuracy: 0.9271\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2554 - accuracy: 0.9298 - val_loss: 0.2624 - val_accuracy: 0.9281\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2527 - accuracy: 0.9303 - val_loss: 0.2668 - val_accuracy: 0.9253\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2508 - accuracy: 0.9309 - val_loss: 0.2701 - val_accuracy: 0.9243\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f85d42cc4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we try to add more layers and do a better logistic regression. We can so add intermidiet layers. Now we use tree layers. And i can choose the size. Softmax is usally used only for the final layes. Relu is used for intermediet layers. You can specify an activation as an additional layer if you want."
      ],
      "metadata": {
        "id": "TzfJNR3pqOQA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0mBuorMulG5"
      },
      "source": [
        "xin = Input(shape=(784))\n",
        "x = Dense(128,activation='relu')(xin)\n",
        "res = Dense(10,activation='softmax')(x)\n",
        "# X2 = Dense(100)(x)\n",
        "# X3 = Activation(activation = \"relu\")(X2)\n",
        "\n",
        "mynet2 = Model(inputs=xin,outputs=res)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpMyvw7buzhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430c677b-71f4-4570-d9d8-27e6763d0073"
      },
      "source": [
        "mynet2.summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 784)]             0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               100480    \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYg0odW2u6cn"
      },
      "source": [
        "mynet2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDnzgIZVvGOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f257f23b-8811-4089-f612-d99592406bef"
      },
      "source": [
        "mynet2.fit(x_train,y_train_cat, shuffle=True, epochs=10, batch_size=32,validation_data=(x_test,y_test_cat))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2576 - accuracy: 0.9265 - val_loss: 0.1317 - val_accuracy: 0.9614\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1148 - accuracy: 0.9667 - val_loss: 0.1025 - val_accuracy: 0.9694\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0780 - accuracy: 0.9768 - val_loss: 0.0895 - val_accuracy: 0.9719\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0588 - accuracy: 0.9823 - val_loss: 0.0833 - val_accuracy: 0.9742\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0442 - accuracy: 0.9868 - val_loss: 0.0833 - val_accuracy: 0.9751\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0363 - accuracy: 0.9890 - val_loss: 0.0696 - val_accuracy: 0.9793\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0285 - accuracy: 0.9913 - val_loss: 0.0727 - val_accuracy: 0.9779\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0234 - accuracy: 0.9929 - val_loss: 0.0842 - val_accuracy: 0.9751\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0189 - accuracy: 0.9944 - val_loss: 0.0830 - val_accuracy: 0.9774\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.0814 - val_accuracy: 0.9774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f85d41e8490>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcUKxCzKwzG9"
      },
      "source": [
        "An amazing improvement. WOW!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mynet.save_weights(\"my_file\")\n",
        "mynet2.save_weights(\"my_file\")"
      ],
      "metadata": {
        "id": "GcwyJF9xsOr8"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJr0EnAkw6nf"
      },
      "source": [
        "# Exercises\n",
        "\n",
        "1.   Add additional Dense layers and check the performance of the network\n",
        "2.   Replace 'relu' with different activation functions\n",
        "3. Adapt the network to work with the so called sparse_categorical_crossentropy\n",
        "4. the fit function return a history of training, with temporal sequences for all different metrics. Make a plot.\n",
        "\n",
        "\\\\"
      ]
    }
  ]
}